{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_yqToottmrB"
      },
      "source": [
        "# **Fraud Detection Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TCvPFxytmrE"
      },
      "source": [
        "### Importing Necassary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-Xma-qvtmrF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn import metrics, model_selection\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Import libraries used for Logistic Regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5u2ADK5tmrG"
      },
      "outputs": [],
      "source": [
        "# Retrieving the dataset\n",
        "df = pd.read_csv(\"fraud_data1.csv\", na_values='?')\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Id0dzvtmrG"
      },
      "source": [
        "### Correlation Matrix Between Features Using HeatMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "foXRvVAetmrG",
        "outputId": "0fc9462b-6470-484c-a0ed-718143cb4cbb"
      },
      "outputs": [],
      "source": [
        "# Correlation table\n",
        "correlation = df.corr()\n",
        "correlation[\"Class\"].sort_values(ascending=False)\n",
        "# Plotting Corr. Matrix\n",
        "plt.figure(figsize=(20, 9))\n",
        "plt.title(\"Credit Card Transactions Features Correlation Plot\", fontsize=18)\n",
        "sns.heatmap(\n",
        "    correlation,\n",
        "    xticklabels=correlation.columns,\n",
        "    yticklabels=correlation.columns,\n",
        "    linewidth=2,\n",
        "    cmap=\"Oranges\",\n",
        "    cbar=True,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t1MDYI7tmrI"
      },
      "source": [
        "### Eliminating features whose correlation value is greater than 0.75 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZsVqoc6tmrI",
        "outputId": "745ea7e9-07f2-4ca2-f011-54a72a98e542"
      },
      "outputs": [],
      "source": [
        "columns = list(df.columns)\n",
        "for column in columns:\n",
        "    # Skip Class column\n",
        "    if column == \"Class\":\n",
        "        continue\n",
        "    filtered_columns = [column]\n",
        "    # Iterate through HeatMap\n",
        "    for col in df.columns:\n",
        "        # Skip the diagonals and the class columns\n",
        "        if (column == col) | (column == \"Class\"):\n",
        "            continue\n",
        "        # Retrieving corr val from df\n",
        "        cor_val = df[column].corr(df[col])\n",
        "        # Setting threshold to be 70%\n",
        "        if cor_val > 0.75:\n",
        "            columns.remove(col)\n",
        "            continue\n",
        "        else:\n",
        "            filtered_columns.append(col)\n",
        "    # Keeping only filtered columns\n",
        "    df = df[filtered_columns]\n",
        "\n",
        "features = df.drop([\"Class\"], axis=1)\n",
        "# We are gonig to scale the featrues with MinMax Scaler\n",
        "mm_scaler = MinMaxScaler((0,1))\n",
        "X = mm_scaler.fit_transform(features)\n",
        "\n",
        "#<--> Lines (31-33) Cited\n",
        "\n",
        "# Using Chi Squared to filter columns\n",
        "selector = SelectKBest(chi2, k=21)\n",
        "selector.fit(X, df[\"Class\"])\n",
        "filtered_columns = selector.get_support()\n",
        "\n",
        "# Adding Class column back to dataframe\n",
        "filtered_data = features.loc[:, filtered_columns]\n",
        "df = filtered_data.join(df[\"Class\"])\n",
        "\n",
        "print(\"After filtering highly correlated features, dataset is \", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-XOJry1tmrJ"
      },
      "source": [
        "### Observation\n",
        "Let's observe the dataset and find connection between fraudulant and normal transactions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-zedV0WtmrK",
        "outputId": "807c0998-e428-4f7e-9e24-73781db8ccb5"
      },
      "outputs": [],
      "source": [
        "#temp_df = df.sort_values(by = [\"Time\"])\n",
        "frauds = df.query(\"Class == 1\")\n",
        "normals = df.query(\"Class == 0\")\n",
        "total = len(df)\n",
        "fraud_percent = round((len(frauds) / total)*100, 2)\n",
        "print(\"The percentage of fraudulant transactions is: \", fraud_percent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW_GNCEhtmrK"
      },
      "source": [
        "As the percentage of fraudulant transactions is trivial making the dataset strongly imbalanced,so we might apply the following: \n",
        "1. Increase the number of samples from minority class\n",
        "2. Decrease the number of samples from majority class \\\n",
        "\\\n",
        "We can also penalyze the algorithm to achieve better scores for the following metrics: Confusion Matrix, Precision, Recall, F1-score, AOC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "7wbKv2Q6tmrL",
        "outputId": "40af3cc1-5a38-412b-c9b7-0719ef636cb1"
      },
      "outputs": [],
      "source": [
        "# Function to plot based on the passed arguments\n",
        "def plot_line(x, y, title=\"\", xlabel=\"\", ylabel=\"\", color=\"\"):\n",
        "    plt.plot(x, y, color)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel(xlabel, fontsize=12)\n",
        "    plt.ylabel(ylabel, fontsize=12)\n",
        "    plt.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0, 0))\n",
        "    plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
        "\n",
        "plt.figure(figsize=(16, 7))\n",
        "# Normal transactions on amount vs time chart\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_line(\n",
        "    x=normals[\"Time\"],\n",
        "    y=normals[\"Amount\"],\n",
        "    title=\"Normal Transactions over Time\",\n",
        "    xlabel=\"Time [s]\",\n",
        "    ylabel=\"Transactions Amount\",\n",
        "    color=\"g-o\",\n",
        ")\n",
        "# Fraudulant transactions on amount vs time chart\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_line(\n",
        "    x=frauds[\"Time\"],\n",
        "    y=frauds[\"Amount\"],\n",
        "    title=\"Fraudulent Transactions over Time\",\n",
        "    xlabel=\"Time [s]\",\n",
        "    ylabel=\"Transactions Amount\",\n",
        "    color=\"r-o\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CitSD_QktmrL"
      },
      "outputs": [],
      "source": [
        "std_scaler = StandardScaler()\n",
        "# Retrieving amount values to rescale\n",
        "amount_vals = df['Amount'].values\n",
        "# Standard Scaling amount values and reshaping as a column.\n",
        "df['Amount'] = std_scaler.fit_transform(amount_vals.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOgmgbv1tmrL"
      },
      "source": [
        "### Splitting Data & Creating Training Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsOBErxRtmrM",
        "outputId": "a157bec1-e7a2-447e-929c-7d54b6aa4054"
      },
      "outputs": [],
      "source": [
        "   # As the dataset is huge and it takes forever to run our models, \n",
        "# we're going to reduce the sample size to 50k choosing indices randomly\n",
        "indices = np.array(df.loc[df['Class'] == 0].index)\n",
        "rands = np.random.choice(indices, 19502, replace = False)\n",
        "random_normals = df.iloc[rands, :]\n",
        "frames = [frauds, random_normals]\n",
        "df = pd.concat(frames)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "y = np.array(df['Class'])\n",
        "X = np.array(df.loc[:, df.columns != 'Class'])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=42, shuffle=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state=42, shuffle=True)\n",
        "\n",
        "print(\"Before oversampling the data: \")\n",
        "print(\"X_train Shape: \", X_train.shape)\n",
        "print(\"X_test Shape: \", X_val.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "# We are going to oversample the dataset and create new synthetic data samples resembling the minority class\n",
        "ros = RandomOverSampler(sampling_strategy=\"minority\", random_state=1)\n",
        "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "#X_over_test, y_over_test = ros.fit_resample(X_test, y_test)\n",
        "\n",
        "print(\"After oversampling the data: \")\n",
        "print(\"X_over_train Shape: \", X_train.shape)\n",
        "print(\"X_test Shape: \", X_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpOm6s4StmrM"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "fOKqS5aZtmrM",
        "outputId": "15bdefe2-5efd-4c77-c9eb-c99e07732a17"
      },
      "outputs": [],
      "source": [
        "# Let's start by testing logistic regression accuracy without regularization on a variety of different feature transformations\n",
        "for i in range(1, 4):\n",
        "    poly = PolynomialFeatures(i)\n",
        "    X_train_new = poly.fit_transform(X_train)\n",
        "    X_val_new = poly.fit_transform(X_val)\n",
        "    logreg = LogisticRegression(max_iter=100000, penalty='none')\n",
        "    logreg.fit(X_train_new, y_train)\n",
        "    y_pred_train = logreg.predict(X_train_new)\n",
        "    y_pred_val = logreg.predict(X_val_new)\n",
        "    print(f'Validation Recall of logistic regression, no regularization, and transformation X^{i}: {recall_score(y_val, y_pred_val)}')\n",
        "    print(f'Train Recall of logistic regression, no regularization, and transformation X^{i}: {recall_score(y_train, y_pred_train)}')\n",
        "\n",
        "# We see that polynomial transformation of degree 1 (i.e no transformation) performs best, \n",
        "# so will use this for the next step.\n",
        "\n",
        "# The next step will be using K-fold cross validation to help us identify the best hyperparameters\n",
        "# as well as see if we can get our model's performance to improve and decrease overfitting by\n",
        "# adding regularization.\n",
        "\n",
        "best_score = [0, 0, 0]\n",
        "best_y_pred = None\n",
        "\n",
        "print(\"--------------------\")\n",
        "C = [0.001, 0.1, 1, 10, 100, 1000, 10000]\n",
        "for k in range(4):\n",
        "    L2_logreg = LogisticRegressionCV(Cs=C, cv=k+3, max_iter=100000)\n",
        "    L2_logreg.fit(X_train, y_train)\n",
        "    y_pred = L2_logreg.predict(X_val)\n",
        "    score = recall_score(y_val, y_pred)\n",
        "    print(f'Logistic Regression Accuracy with K^{k+3}, L2 Regularization and C={max(L2_logreg.C_)} is: {score}')\n",
        "\n",
        "    if score > best_score[0]:\n",
        "        best_score[0], best_score[1] = score, max(L2_logreg.C_)\n",
        "        best_score[2] = 2\n",
        "        best_y_pred = y_pred\n",
        "\n",
        "    L1_logreg = LogisticRegressionCV(Cs=C, cv=k+3, max_iter=100000, penalty='l1', solver='liblinear')\n",
        "    L1_logreg.fit(X_train, y_train)\n",
        "    y_pred = L1_logreg.predict(X_val)\n",
        "    score = recall_score(y_val, y_pred)\n",
        "    print(f'Logistic Regression Accuracy with K^{k+3}, L1 Regularization and C={max(L1_logreg.C_)} is: {score}')\n",
        "\n",
        "    if score > best_score[0]:\n",
        "        best_score[0], best_score[1] = score, max(L1_logreg.C_)\n",
        "        best_score[2] = 1\n",
        "        best_y_pred = y_pred\n",
        "\n",
        "print(\"-------------\")\n",
        "print(f\"Our best results were L{best_score[2]} Regularization with C={best_score[1]} and score = {best_score[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "jULHfGK6tmrM",
        "outputId": "71e096c0-2abc-4f59-a1cf-0032e6786d77"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "\n",
        "cm = metrics.confusion_matrix(y_val, best_y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "all_sample_title = f'Recall Score: {best_score[0]}'\n",
        "plt.title(all_sample_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw3KrgJZtmrN",
        "outputId": "ef301f00-4564-4b47-8ab6-fb155b25e04e"
      },
      "outputs": [],
      "source": [
        "print('Precision: %.3f' % precision_score(y_val, best_y_pred))\n",
        "print('Recall: %.3f' % recall_score(y_val, best_y_pred))\n",
        "print('Accuracy: %.3f' % accuracy_score(y_val, best_y_pred))\n",
        "print('F1-Score: %.3f' % f1_score(y_val, best_y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl_YBwSMtmrN"
      },
      "source": [
        "### Support Vector Machine (SVM)\n",
        "\n",
        "For the SVM, we will try a variety of feature transformations and regularization techniques using sklearn's SVM library including the following:\n",
        "- Linear Kernel and L1 (Lasso regression)\n",
        "- Polynomial, degree 2, L2 regularization\n",
        "- Gaussian RBF with L2 regularization\n",
        "\n",
        "We will make a plot of the test and train accuracies for all of this variation across different choices of the hyperparamter lambda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj1TujOGtmrO"
      },
      "outputs": [],
      "source": [
        "# import SVC classifier, \n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "# Initialize some lambda values to validation across SVM choices\n",
        "my_lambdas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxzGua9ptmrO"
      },
      "source": [
        "We'll start off by testing variations of linear SVM with lasso regression (L1 regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "pkV-YwI2tmrO",
        "outputId": "8a172de4-2834-48d0-8f08-c42998359117"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "val_results, train_results = [], []\n",
        "\n",
        "\n",
        "for C in my_lambdas:\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')\n",
        "        svm = LinearSVC(C=C, loss='hinge')\n",
        "        svm.fit(X_train,y_train)\n",
        "\n",
        "        y_pred_train = svm.predict(X_train)\n",
        "        y_pred_val = svm.predict(X_val)\n",
        "        \n",
        "        train_score = recall_score(y_train, y_pred_train)\n",
        "        val_scores = recall_score(y_val, y_pred_val)\n",
        "        print(f\"C={C}, Train Recall = {train_score}, Validation Recall ={val_scores}\")\n",
        "\n",
        "    train_results.append(train_score)\n",
        "    val_results.append(val_scores)\n",
        "    \n",
        "\n",
        "plt.xscale(\"log\")\n",
        "plt.plot(my_lambdas, train_results, \"hotpink\", label=\"train\")\n",
        "plt.plot(my_lambdas, val_results, \"teal\", label=\"val\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"Values of C\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"SVM with Linear Kernel and L1 Lasso\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0wOplb-tmrP"
      },
      "source": [
        "We can move onto testing a new kernel function, polynomial of degree 2 because we already have lots of features. This will also use L2 regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "nz9HqgpgtmrP",
        "outputId": "140e3725-d81c-462a-83c4-bf8d0d7e60d0"
      },
      "outputs": [],
      "source": [
        "val_results, train_results = [], []\n",
        "\n",
        "for C in my_lambdas:\n",
        "    svm = SVC(C=C, kernel='poly', degree=2) # L2 regularization added by default\n",
        "    svm.fit(X_train,y_train)\n",
        "    \n",
        "    y_pred_train = svm.predict(X_train)\n",
        "    y_pred_val = svm.predict(X_val)\n",
        "    \n",
        "    train_score = recall_score(y_train, y_pred_train)\n",
        "    val_score = recall_score(y_val, y_pred_val)\n",
        "    print(f\"C={C}, Train Recall = {train_score}, Validation Recall = {val_score}\")\n",
        "\n",
        "    train_results.append(train_score)\n",
        "    val_results.append(val_score)\n",
        "\n",
        "plt.xscale(\"log\")\n",
        "plt.plot(my_lambdas, train_results, \"hotpink\", label=\"train\")\n",
        "plt.plot(my_lambdas, val_results, \"teal\", label=\"val\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"Values of C\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"SVM with Polynomial Kernel and L2 Ridge\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFZ7qXTUtmrP"
      },
      "source": [
        "Finally, we'll last test the Gaussian RBF kernel transformation with ridge regression (L2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRSyb185tmrQ"
      },
      "outputs": [],
      "source": [
        "val_results, train_results = [], []\n",
        "\n",
        "for C in my_lambdas:\n",
        "    svm = SVC(C=C) # Default SVC uses RBF and L2\n",
        "    svm.fit(X_train,y_train)\n",
        "    \n",
        "    y_pred_train = svm.predict(X_train)\n",
        "    y_pre_val = svm.predict(X_val)\n",
        "    \n",
        "    train_score = recall_score(y_train, y_pred_train)\n",
        "    val_score = recall_score(y_val, y_pre_val)\n",
        "    print(f\"C={C}, Train Recall = {train_score}, Validation Recall = {val_score}\")\n",
        "\n",
        "    train_results.append(train_score)\n",
        "    val_results.append(val_score)\n",
        "    \n",
        "plt.xscale(\"log\")\n",
        "plt.plot(my_lambdas, train_results, \"hotpink\", label=\"train\")\n",
        "plt.plot(my_lambdas, val_results, \"teal\", label=\"val\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"Values of C\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"SVM with RBF and L2 Ridge\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZB_hMsl4Gk8"
      },
      "source": [
        "### Function to Plot by Passing Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t62kHi24HKM"
      },
      "outputs": [],
      "source": [
        "def save_figure(C, train_results, val_results, title, path):\n",
        "    plt.xscale(\"log\")\n",
        "    plt.plot(C, train_results, \"hotpink\", label=\"train\")\n",
        "    plt.plot(C, val_results, \"teal\", label=\"val\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.xlabel(\"Values of C\")\n",
        "    plt.ylabel(\"Recall\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(path+'/'+title, dpi=500)\n",
        "    plt.close()\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrb7aqqutmrQ"
      },
      "source": [
        "### Artificial Neural Network (ANN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8RVrSvHtmrQ",
        "outputId": "041dff76-4888-49fc-f630-2766b26cc4c6"
      },
      "outputs": [],
      "source": [
        "# ANN with 1 hidden layers with sizes of (200, )\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import regularizers, metrics, backend\n",
        "import os\n",
        "\n",
        "# Declaring path to figure directory\n",
        "curr_dir = os.getcwd()\n",
        "figure_path = curr_dir+'/'+\"ANN Figures\"\n",
        "\n",
        "C = [0.1, 10, 1000, 100000]\n",
        "\n",
        "scores = pd.DataFrame(columns = (\"C\", \"0.1\", \"10\", \"1000\", \"100000\"))\n",
        "best_recall = 0\n",
        "best_model_data = []\n",
        "best_y_pred = ...\n",
        "\n",
        "# We're going to use only ReLU activation as sigmoid and others performed poorly\n",
        "# Input layer params\n",
        "input_layers = [(\"Input 200\", 200, 21, regularizers.L2, \"L2\"),\n",
        "                #(\"Input 200\", 200, 21, regularizers.L1, \"L1\"),\n",
        "                #(\"Input 250\", 250, 21, regularizers.L2, \"L2\")\n",
        "                (\"Input 250\", 250, 21, regularizers.L1, \"L1\")\n",
        "                ]\n",
        "# Hidden layer params\n",
        "hidden_layers = [(\"None\", None, None, \"None\"),\n",
        "                (\"Hidden 100\", 100,  regularizers.L1, \"L1\"),\n",
        "                #(\"Hidden 150\", 150,  regularizers.L1, \"L1\"),\n",
        "                #(\"Hidden 100\", 100,  regularizers.L2, \"L2\"),\n",
        "                (\"Hidden 150\", 150,  regularizers.L2, \"L2\")\n",
        "                ]\n",
        "\n",
        "# We need to use Sequential model to add new layers and experiment\n",
        "model = Sequential()\n",
        "epochs = 2\n",
        "\n",
        "for input_index, (input_name, input_layer_size, input_size, input_reg, ireg_name) in enumerate(input_layers):\n",
        "    \n",
        "    for hidden_index, (hidden_name, hidden_layer_size, hidden_reg, hreg_name) in enumerate(hidden_layers):\n",
        "        model_state = []\n",
        "        model_state.append(str(input_layer_size)+\", \"+str(hidden_layer_size)+\"\\n\"+ireg_name+\" \"+hreg_name)\n",
        "        name = \"{}, Reg: {} Hidden: {}, Reg: {}\".format(input_name, ireg_name, hidden_name, hreg_name)\n",
        "        val_results, train_results = [], []\n",
        "        # Trying different C values\n",
        "        for c_val in C:\n",
        "            # Adding an input layer with params from input_layers\n",
        "            model.add(Dense(input_layer_size, input_shape=(input_size,), kernel_regularizer = input_reg(1/c_val), activation='relu', name=\"layer1\"))\n",
        "\n",
        "            if hidden_name != \"None\":\n",
        "                # Adding hidden layer with params from hidden_layers\n",
        "                model.add(Dense(hidden_layer_size,  kernel_regularizer = hidden_reg(1/c_val), activation='relu', name=\"layer2\"))\n",
        "            # Adding Output Layer\n",
        "            model.add(Dense(1, activation='sigmoid', name=\"layer3\"))\n",
        "\n",
        "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', metrics.Precision(), metrics.Recall()])\n",
        "            model.fit(X_train, y_train, epochs=epochs, batch_size=10)\n",
        "            print(\"C Value: \", c_val)\n",
        "            train_results.append(model.evaluate(X_train, y_train)[3])\n",
        "            results = model.evaluate(X_val, y_val)\n",
        "            val_results.append(results[3])\n",
        "            # Chosing best model based on recall / Full Report\n",
        "            if results[3] > best_recall and results[3] < 0.9999:\n",
        "                best_recall = results[3]\n",
        "                name = \"{}, Reg: {} Hidden: {}, Reg: {}\".format(input_name, ireg_name, hidden_name, hreg_name)\n",
        "                best_model = [name, epochs, c_val, results]\n",
        "                best_y_pred = np.argmax(model.predict(X_val),axis=1)\n",
        "\n",
        "            # Saving recall in model state for current configuration\n",
        "            model_state.append(results[3])\n",
        "            backend.clear_session()\n",
        "            model.pop()\n",
        "            # Pop hidden layer after computation\n",
        "            if hidden_name != \"None\":\n",
        "                model.pop()\n",
        "            model.pop()  \n",
        "        # Adding trained model's state in scores dataframe\n",
        "        scores.loc[len(scores.index)] = model_state\n",
        "        # Saving Figure as an Image\n",
        "        save_figure(C, train_results, val_results, name, figure_path)\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "tGPu-OqxtmrQ",
        "outputId": "5445b183-7f9a-4455-d3fc-83c72659d7a8"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics as mt\n",
        "# Confusion Matrix for Best Model\n",
        "cm = mt.confusion_matrix(y_val, best_y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "all_sample_title = f'Recall Score: {best_model[3][3]}'\n",
        "plt.title(all_sample_title)\n",
        "# Storing scores dataframe to external file\n",
        "scores.to_csv(\"scores_output.csv\",index = False)\n",
        "print(best_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4 (main, Nov  2 2022, 18:29:07) [GCC 9.4.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
